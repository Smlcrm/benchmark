test_type: deterministic
dataset:
  name: australian_electricity_demand
  frequency: D
  split_ratio: [0.8, 0.1, 0.1]
  normalize: false
  handle_missing: interpolate
  chunks: 2

model:
  tiny_time_mixer:
    forecast_horizon: [10]
  moment:
    model_path: ['AutonLab/MOMENT-1-large']
    context_length: [512]
    fine_tune_epochs: [0]
    batch_size: [8]
    learning_rate: [0.0001]
    prediction_length: [10]
  lagllama:
    context_length: [4]
    num_samples: [5]
    batch_size: [4]
    prediction_length: [10]
  timesfm:
    per_core_batch_size: [2]
    horizon_len: [10]
    num_layers: [2]
    context_len: [2]
    use_positional_embedding: [False]
  tabpfn:
    allow_large_cpu_dataset: [True]
    max_sequence_length: [32]
    prediction_length: [10]
  chronos:
    model_size: ['small']
    context_length: [8]
    num_samples: [5]
    prediction_length: [10]
  xgboost:
    lookback_window: [10]
    n_estimators: [10]
    max_depth: [5]
    learning_rate: [0.1]
    random_state: [42]
    n_jobs: [-1]
    forecast_horizon: [10]
  prophet:
    seasonality_mode: ["additive"]
    changepoint_prior_scale: [0.05]
    seasonality_prior_scale: [10.0]
    yearly_seasonality: [false]
    weekly_seasonality: [false]
    daily_seasonality: [false]
  lstm:
    units: [32]
    layers: [1]
    dropout: [0.1]
    learning_rate: [0.01, 1.0]
    batch_size: [32]
    epochs: [20]
    sequence_length: [10]
    training_loss: ["mae"]
    forecast_horizon: [10]
  random_forest:
    lookback_window: [50, 100]
    n_estimators: [10, 50]
    max_depth: [3, 7]
    random_state: [42]
    n_jobs: [-1]
    forecast_horizon: [10]
  exponential_smoothing:
    trend: ["none", "add"]
    seasonal: ["none", "add", "mul"]
    seasonal_periods: [7]
    damped_trend: [false]
    forecast_horizon: [10]
  svr:
    kernel: ["rbf", "linear"]
    C: [0.1, 1.0, 10.0]
    epsilon: [0.01, 0.1, 0.2]
    gamma: ["scale", "auto"]
    lookback_window: [50, 100]
    random_state: [42]
    forecast_horizon: [10]
  arima:
    p: [0, 1, 2]
    d: [0, 1]
    q: [0, 1, 2]
    s: [2, 4]
    training_loss: ["mae"]
    forecast_horizon: [10]
  theta:
    sp: [1, 2, 3]
    forecast_horizon: [10]
  moirai:
    size: ["small"]
    psz: [32]
    bsz: [8]
    num_samples: [5]
    pdt: [10]
  moirai_moe:
    size: ["small"]
    psz: [32]
    bsz: [8]
    num_samples: [5]
    pdt: [10]
  toto:
    num_samples: [40]
    samples_per_batch: [40]
    prediction_length: [10]
  seasonal_naive:
    sp: [7, 14]
    forecast_horizon: [10]
  deepar:
    hidden_size: [10]
    rnn_layers: [2]
    dropout: [0.1]
    learning_rate: [0.001]
    batch_size: [16]
    epochs: [1]
    sequence_length: [10]
    training_loss: ["mae"]
    forecast_horizon: [10]
    max_encoder_length: [6]
    max_prediction_length: [6]
    gradient_clip_val: [0.1]
    num_workers: [7]
    feature_cols: [null]
  lstm:
    units: [32]
    layers: [1]
    dropout: [0.1]
    learning_rate: [0.01, 1.0]
    batch_size: [32]
    epochs: [20]
    sequence_length: [10]
    training_loss: ["mae"]
    forecast_horizon: [10]
    feature_cols: [null]
  random_forest:
    lookback_window: [50, 100]
    n_estimators: [10, 50]
    max_depth: [3, 7]
    random_state: [42]
    n_jobs: [-1]
    forecast_horizon: [10]
    model_params: [{}]
  svr:
    kernel: ["rbf", "linear"]
    C: [0.1, 1.0, 10.0]
    epsilon: [0.01, 0.1, 0.2]
    gamma: ["scale", "auto"]
    lookback_window: [50, 100]
    max_iter: [1000]
    random_state: [42]
    forecast_horizon: [10]
    model_params: [{}]
  prophet:
    seasonality_mode: ["additive"]
    changepoint_prior_scale: [0.05]
    seasonality_prior_scale: [10.0]
    yearly_seasonality: [false]
    weekly_seasonality: [false]
    daily_seasonality: [false]
    forecast_horizon: [10]
    model_params: [{}]
  seasonal_naive:
    sp: [7, 14]
    forecast_horizon: [10]
    model_params: [{"sp": 7}, {"sp": 14}]
  exponential_smoothing:
    trend: ["none", "add"]
    seasonal: ["none", "add", "mul"]
    seasonal_periods: [7]
    damped_trend: [false]
    forecast_horizon: [10]
    training_loss: ["mae"]
  tabpfn:
    allow_large_cpu_dataset: [True]
    max_sequence_length: [32]
    forecast_horizon: [10]
  tiny_time_mixer:
    model_name: ["tiny_time_mixer"]
    forecast_horizon: [10]
  moment:
    model_path: ['AutonLab/MOMENT-1-large']
    context_length: [512]
    fine_tune_epochs: [0]
    batch_size: [8]
    learning_rate: [0.0001]
    forecast_horizon: [10]
  lagllama:
    context_length: [4]
    num_samples: [5]
    batch_size: [4]
    forecast_horizon: [10]
  timesfm:
    per_core_batch_size: [2]
    horizon_len: [10]
    num_layers: [2]
    context_len: [2]
    use_positional_embedding: [False]
    forecast_horizon: [10]
  chronos:
    model_size: ['small']
    context_length: [8]
    num_samples: [5]
    forecast_horizon: [10]
  xgboost:
    lookback_window: [10]
    n_estimators: [10]
    max_depth: [5]
    learning_rate: [0.1]
    random_state: [42]
    n_jobs: [-1]
    forecast_horizon: [10]
    model_params: [{}]
  arima:
    p: [0, 1, 2]
    d: [0, 1]
    q: [0, 1, 2]
    s: [2, 4]
    training_loss: ["mae"]
    forecast_horizon: [10]
  theta:
    sp: [1, 2, 3]
    forecast_horizon: [10]
  moirai:
    model_name: ["moirai"]
    size: ["small"]
    psz: [32]
    bsz: [8]
    num_samples: [5]
    pdt: [10]
    forecast_horizon: [10]
  moirai_moe:
    model_name: ["moirai_moe"]
    size: ["small"]
    psz: [32]
    bsz: [8]
    num_samples: [5]
    pdt: [10]
    ctx: [10]
    target_col: ["y"]
    forecast_horizon: [10]
  toto:
    model_name: ["toto"]
    num_samples: [40]
    samples_per_batch: [40]
    forecast_horizon: [10]
  croston_classic:
    alpha: [0.4]
    beta: [0.1]
    gamma: [0.1]
    phi: [0.9]
    forecast_horizon: [10]

evaluation:
  type: deterministic
  metrics: [mae, mase, rmse]